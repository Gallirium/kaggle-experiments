{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583bf12e-87fd-4c67-b7aa-c4e92dcd1360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallirium/dostNN/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from corus import load_wikiner\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda629e9-4970-4995-ad2f-a74cd4af63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = load_wikiner('./aij-wikiner-ru-wp3.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6453f80-cdc1-4610-8bb4-2b18ddef8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = []\n",
    "possible_ner_tags = set()\n",
    "for item in datagen:\n",
    "    d = {'sent': [], 'tags': []}\n",
    "    for x in item.tokens:\n",
    "        d['sent'].append(x.text)\n",
    "        d['tags'].append(x.tag)\n",
    "        ## Кажется, можно было бы достать возможные теги лучше...\n",
    "        if x.tag not in possible_ner_tags:\n",
    "            possible_ner_tags.add(x.tag)\n",
    "    dict.append(d)\n",
    "ner_list = list(possible_ner_tags)\n",
    "ner_list.remove(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1635d23-c97e-4093-9c66-6d050e287004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'I-ORG': 1, 'I-LOC': 2, 'I-MISC': 3, 'B-MISC': 4, 'B-LOC': 5, 'I-PER': 6, 'B-ORG': 7, 'B-PER': 8}\n"
     ]
    }
   ],
   "source": [
    "ner_tag_to_idx = {}\n",
    "ner_tag_to_idx['O'] = 0\n",
    "for idx,tag in enumerate(ner_list):\n",
    "    ner_tag_to_idx[tag] = idx+1\n",
    "#ner_tag_to_idx = {tag: idx+1 for idx, tag in enumerate(ner_list)}\n",
    "print(ner_tag_to_idx)\n",
    "\n",
    "ner_idx_to_tag = {v: k for k,v in ner_tag_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bccd6a83-0aed-4ae2-833f-cef3243102fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dict:\n",
    "    d['ner_tags'] = [ner_tag_to_idx[tag] for tag in d['tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0abb91-6317-40a1-b7d5-9f662f1b767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de6f231-999b-4f6b-bd57-eaa1a42f9261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2988, 14576, 24340, 869, 105058, 128, 1469, 12266, 130, 130, 869, 69981, 128, 1469, 9059, 130, 13124, 130, 130, 236, 49322, 851, 37210, 33424, 3590, 132, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'На',\n",
       " 'севере',\n",
       " 'граничит',\n",
       " 'с',\n",
       " 'Латвией',\n",
       " ',',\n",
       " 'на',\n",
       " 'востоке',\n",
       " '-',\n",
       " '-',\n",
       " 'с',\n",
       " 'Белоруссией',\n",
       " ',',\n",
       " 'на',\n",
       " 'юго',\n",
       " '-',\n",
       " 'западе',\n",
       " '-',\n",
       " '-',\n",
       " 'c',\n",
       " 'Польшей',\n",
       " 'и',\n",
       " 'Калининградской',\n",
       " 'областью',\n",
       " 'России',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = tokenizer(dict[0]['sent'],truncation=True, is_split_into_words=True)\n",
    "print(toks)\n",
    "tokenizer.convert_ids_to_tokens(toks['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4798b53-587b-4d94-b9cf-f2aa9ad315c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2988, 14576, 24340, 869, 105058, 128, 1469, 12266, 130, 130, 869, 69981, 128, 1469, 9059, 130, 13124, 130, 130, 236, 49322, 851, 37210, 33424, 3590, 132, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bec21a4-a583-491f-a7e6-6682c634c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(dict):\n",
    "    dataset = []\n",
    "    for i in range(len(dict)):\n",
    "        toks = tokenizer(dict[i]['sent'], truncation=True, is_split_into_words=True)\n",
    "        \n",
    "        token_ner = []\n",
    "        for idx in toks.word_ids():\n",
    "            if idx is None: token_ner.append(-100)\n",
    "            else:\n",
    "                token_ner.append(dict[i]['ner_tags'][idx])\n",
    "        toks['labels'] = token_ner\n",
    "        dataset.append(toks)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cfc094b-d622-49db-a9d5-8667a1691e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = align(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c97814ab-87b2-4d0d-a21f-e7f522171132",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Для динамического пэддинга предложений\n",
    "collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ccd279-c63b-4627-8950-8e713cbe2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    true_preds = [[ner_idx_to_tag[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "    true_labels = [[ner_idx_to_tag[l] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "\n",
    "    res = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\"precision\": res[\"overall_precision\"],\n",
    "            \"recall\": res[\"overall_recall\"],\n",
    "            \"f1\": res[\"overall_f1\"],\n",
    "            \"accuracy\": res[\"overall_accuracy\"],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "907217ae-7b31-41a5-be12-fce5add76d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir = 'russian_ner_test',\n",
    "    learning_rate = 3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.02,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "237f5992-5a0c-43da-b486-8110e09ff34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=len(ner_idx_to_tag))\n",
    "model.config.id2label = ner_idx_to_tag\n",
    "model.config.label2id = ner_tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4950a2fb-86cc-4ea2-8efc-34a0b531d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain, dtest = train_test_split(dict, test_size=0.2, random_state=42)\n",
    "\n",
    "ner_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(dtrain),\n",
    "    \"test\": Dataset.from_list(dtest)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78017d2f-bfdc-4cf8-9f07-88584fd9f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=ner_dataset[\"train\"],\n",
    "    eval_dataset=ner_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1efaf0-4ca8-4d7f-a77d-f75eb7350ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20330' max='20329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20329/20329 34:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3873' max='5083' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3873/5083 01:08 < 00:21, 56.47 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba115b6-3d26-4cf4-8cf1-b75aeecaf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "wnut = load_dataset(\"wnut_17\")\n",
    "example = wnut['train'][0]\n",
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e852f91-c3a3-4072-bb88-acc6763baaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def inference(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    logits = model(**inputs).logits\n",
    "    preds = torch.argmax(logits, dim=2)\n",
    "    class_list = [model.config.id2label[t.item()] for t in preds[0]]\n",
    "    print(text.split(' '))\n",
    "    print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c171657-ec71-4332-997f-ec6f16d252f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(\"Мой Китик самый лучший на планете Земля и совсем скоро выздоровеет)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435779d-3c26-4ae0-87c3-4ff87ad22e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dostNN",
   "language": "python",
   "name": "dostnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
